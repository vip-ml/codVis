<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Curse of Dimensionality Visualization</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <link rel="stylesheet" href="styles.css">
</head>

<body class="bg-gray-50 text-gray-800 antialiased">

    <div class="container mx-auto p-4 md:p-8 max-w-6xl">
        <header class="mb-12">
            <h1 class="text-4xl font-bold text-gray-900 mb-4">Why Machine Learning Models Struggle with High-Dimensional
                Data</h1>
            <p class="text-xl text-gray-600 leading-relaxed">An interactive exploration of the curse of dimensionality
            </p>
        </header>

        <article class="prose prose-lg max-w-none">
            <section class="mb-12">
                <p class="text-lg leading-relaxed text-gray-700 mb-6">
                    You've probably heard the term "curse of dimensionality" thrown around in machine learning
                    discussions,
                    but what does it actually mean? Let's see something that might surprise you.
                </p>

                <p class="text-gray-700 mb-6">
                    Imagine you're looking for your lost wallet in a hallway (1D)—you only need to check in front of you
                    and behind you. Now imagine you've misplaced it in a room (2D); suddenly, you have to search left,
                    right, forward,
                    and backward. What if it's lost in a multi-story building (3D)? The search now includes looking
                    above and below, across different floors. Each added dimension increases the complexity of
                    your search. Now picture trying to find the wallet in a space with 10, 50, or even 100 dimensions.
                    The pursuit becomes exponentially more difficult in this high-dimensional hyperspace.
                    This mirrors what happens when each new attribute in a dataset adds a new dimension—making it
                    progressively harder to search, learn from, or generalize across the feature space.
                </p>

                <p class="text-gray-700 mb-6">
                    In this visualization, data is generated from <strong>multivariate normal distributions</strong> of
                    increasing dimensionality: a 1D normal distribution for the line,
                    a 2D Gaussian for the plane, a 3D Gaussian for the space, and so on for higher dimensions. Each
                    point represents a sample from a <em>d</em>-dimensional normal distribution,
                    and we evaluate how many of these points fall within a hypersphere of radius <em>r</em> centered at
                    the origin. You can experiment with the <strong>number of samples</strong> (i.e., dataset size) and
                    the <strong>search space</strong>, defined in terms of standard deviations from the mean.

                </p>

                <div class="bg-blue-50 border-l-4 border-blue-400 p-6 my-8">
                    <h3 class="text-lg font-semibold text-blue-900 mb-2">Try This Experiment</h3>
                    <p class="text-blue-800">
                        Watch how dramatically the percentage of selected points drops
                        as we move from 1D to 20D, even though we're using the exact same data and selection criteria.
                    </p>
                </div>
            </section>
        </article>

        <!-- Interactive Controls -->
        <div class="bg-white p-6 rounded-lg shadow-sm border mb-8">
            <h3 class="text-lg font-medium text-gray-900 mb-4">Controls</h3>
            <div class="grid grid-cols-1 md:grid-cols-2 gap-6">
                <div>
                    <label for="radius-slider" class="block text-sm font-medium text-gray-700 mb-2">
                        Search radius: <span id="radius-value" class="font-semibold text-blue-600">1.0</span> standard
                        deviations
                    </label>
                    <input id="radius-slider" type="range" min="0.1" max="3" step="0.05" value="1.0"
                        class="w-full h-2 bg-gray-200 rounded-lg appearance-none cursor-pointer">
                    <p class="text-xs text-gray-500 mt-1">How far from the center should we look?</p>
                </div>
                <div>
                    <label for="points-slider" class="block text-sm font-medium text-gray-700 mb-2">
                        Dataset size: <span id="points-value" class="font-semibold text-blue-600">2000</span> points
                    </label>
                    <input id="points-slider" type="range" min="500" max="10000" step="100" value="2000"
                        class="w-full h-2 bg-gray-200 rounded-lg appearance-none cursor-pointer">
                    <p class="text-xs text-gray-500 mt-1">Bigger datasets don't solve the fundamental problem</p>
                </div>
            </div>

            <!-- Visualization Section -->
            <section class="mb-12">
                <h2 class="text-2xl font-bold text-gray-900 mb-6">See It For Yourself</h2>

                <div class="grid grid-cols-1 lg:grid-cols-3 gap-6 mb-8">
                    <!-- 1D: The Line -->
                    <div class="chart-container flex flex-col">
                        <div class="text-center mb-4">
                            <div class="percentage-display" id="p1d">0%</div>
                            <h3 class="text-lg font-medium text-gray-800">1D Gaussian</h3>
                            <p class="text-sm text-gray-600">Standard normal distribution</p>
                        </div>
                        <div id="d3-1d-chart" class="flex-grow min-h-[280px]"></div>
                    </div>

                    <!-- 2D: The Plane -->
                    <div class="chart-container flex flex-col">
                        <div class="text-center mb-4">
                            <div class="percentage-display" id="p2d">0%</div>
                            <h3 class="text-lg font-medium text-gray-800">2D Gaussian</h3>
                            <p class="text-sm text-gray-600">Bivariate normal distribution</p>
                        </div>
                        <div id="d3-2d-chart" class="flex-grow min-h-[280px]"></div>
                    </div>

                    <!-- 3D: The Space -->
                    <div class="chart-container flex flex-col">
                        <div class="text-center mb-4">
                            <div class="percentage-display" id="p3d">0%</div>
                            <h3 class="text-lg font-medium text-gray-800">3D Gaussian</h3>
                            <p class="text-sm text-gray-600">Trivariate normal distribution</p>
                        </div>
                        <div id="d3-3d-chart" class="flex-grow min-h-[280px] relative">
                            <canvas id="three-canvas"></canvas>
                        </div>
                    </div>
                </div>

                <div class="prose prose-lg max-w-none mb-8">
                    <p class="text-gray-700">
                        Notice something interesting? Even going from 2D to 3D, we lose a significant chunk of our data
                        points.
                        This isn't because there's anything wrong with our data or our search radius—it's a fundamental
                        property
                        of high-dimensional spaces.
                    </p>
                </div>
            </section>

            <!-- The Real Problem: Higher Dimensions -->
            <section class="mb-12">
                <h2 class="text-2xl font-bold text-gray-900 mb-6">The Real Problem: High-Dimensional Data in the Wild</h2>

                <p class="text-gray-700 mb-6">
                    The jump from 3 to 10 dimensions is where the curse truly begins to bite. Consider a common
                    scenario in medical research or genomics: you have a dataset with thousands of features (e.g.,
                    gene expression levels) but only a few hundred samples (patients). This is the classic "high p, low
                    n" problem, where 'p' is the number of dimensions (features) and 'n' is the number of samples.
                </p>

                <p class="text-gray-700 mb-6">
                    In this scenario, the feature space is so vast and the data so sparse that finding meaningful
                    patterns becomes statistically challenging. Models are prone to overfitting, mistaking random noise
                    for a genuine signal. As you can see below, even with a modest increase in dimensions, the data
                    scatters into near-nothingness.
                </p>

                <div class="grid grid-cols-2 md:grid-cols-4 gap-4 mb-8">
                    <div class="bg-white p-4 rounded-lg shadow-sm border text-center">
                        <div class="percentage-display text-2xl" id="p4d">0%</div>
                        <div class="text-sm font-medium text-gray-700">4 Dimensions</div>
                        <div class="text-xs text-gray-500 mt-1">Getting sparse</div>
                    </div>
                    <div class="bg-white p-4 rounded-lg shadow-sm border text-center">
                        <div class="percentage-display text-2xl" id="p5d">0%</div>
                        <div class="text-sm font-medium text-gray-700">5 Dimensions</div>
                        <div class="text-xs text-gray-500 mt-1">Critically sparse</div>
                    </div>
                    <div class="bg-white p-4 rounded-lg shadow-sm border text-center">
                        <div class="percentage-display text-2xl" id="p10d">0%</div>
                        <div class="text-sm font-medium text-gray-700">10 Dimensions</div>
                        <div class="text-xs text-gray-500 mt-1">A statistical ghost town</div>
                    </div>
                    <div class="bg-white p-4 rounded-lg shadow-sm border text-center">
                        <div class="percentage-display text-2xl" id="p20d">0%</div>
                        <div class="text-sm font-medium text-gray-700">20 Dimensions</div>
                        <div class="text-xs text-gray-500 mt-1">Effectively empty</div>
                    </div>
                </div>

                <div class="bg-blue-50 border border-blue-200 rounded-lg p-6">
                    <h3 class="text-lg font-medium text-blue-800 mb-2">The Impact on Machine Learning</h3>
                    <p class="text-blue-700">
                        This sparsity forces machine learning models to work much harder. With fewer data points in any
                        given region, it becomes difficult to learn a function that generalizes well. Algorithms that
                        rely on local neighborhoods, like k-NN, suffer the most. Even powerful models like neural
                        networks require vast amounts of data or careful regularization to avoid simply memorizing the
                        training set.
                    </p>
                </div>
            </section>

            <!-- An Interesting Consequence -->
            <section class="mb-12">
                <h2 class="text-2xl font-bold text-gray-900 mb-6">A Weird Geometry Fact</h2>

                <div class="bg-yellow-50 border border-yellow-200 rounded-lg p-6">
                    <h3 class="text-lg font-medium text-yellow-800 mb-2">Distance Becomes Meaningless</h3>
                    <p class="text-yellow-700">
                        Here’s a truly strange consequence of high dimensions: the concept of "close" and "far" breaks
                        down. In a 100-dimensional space, the distance between any two random points becomes almost the
                        same. This is a disaster for algorithms like k-NN, which assume that "close" points are similar.
                        When all points are equally distant, the neighborhood becomes meaningless.
                    </p>
                </div>
            </section>

            <!-- Why This Happens -->
            <section class="mb-12">
                <h2 class="text-2xl font-bold text-gray-900 mb-6">Why Does This Happen? The Geometry of Emptiness</h2>

                <div class="prose prose-lg max-w-none">
                    <p class="text-gray-700 mb-6">
                        The math behind this is both elegant and mind-bending. As we add dimensions, the volume of the
                        space expands exponentially. Think of a tiny cube. In 2D, it's a square. In 3D, it's a cube. In
                        4D and beyond, it's a hypercube. The volume of this hypercube grows incredibly fast, but the
                        number of data points we have doesn't. The data becomes a fine dust scattered across a vast,
                        empty void.
                    </p>

                    <p class="text-gray-700 mb-6">
                        This leads to the famous "orange" analogy. In high dimensions, nearly all the volume of a
                        hypersphere (an n-dimensional sphere) is concentrated in a thin layer near its surface. The
                        juicy "flesh" at the center is almost non-existent. Our data points, scattered randomly, end up
                        in this outer "peel," far from the center and far from each other.
                    </p>
                </div>
            </section>

            <!-- Real-World Impact on Neural Networks -->
            <section class="mb-12">
                <h2 class="text-2xl font-bold text-gray-900 mb-6">Why This Wreaks Havoc on Machine Learning</h2>

                <div class="prose prose-lg max-w-none">
                    <p class="text-gray-700 mb-6">
                        This isn't just a theoretical curiosity; it has brutal consequences for machine learning. For a
                        model to learn a reliable decision boundary, it needs to see enough data to distinguish signal
                        from noise. But in a sparse, high-dimensional space, the model can get fooled. It might draw a
                        complex boundary that perfectly separates the few data points it has seen, but this boundary will
                        be useless for new, unseen data. This is called overfitting.
                    </p>

                    <p class="text-gray-700 mb-6">
                        Deep neural networks, despite their power, are not immune. Their ability to learn complex
                        patterns depends on having enough data to populate the high-dimensional feature spaces they
                        create. Without it, they fall into the same trap, requiring massive datasets or sophisticated
                        regularization techniques to stay on track.
                    </p>

                    <div class="bg-orange-50 border border-orange-200 rounded-lg p-6 mb-6">
                        <h3 class="text-lg font-medium text-orange-800 mb-2">The Data Hunger Problem</h3>
                        <p class="text-orange-700">
                            The amount of data needed to maintain the same density of samples grows exponentially with
                            the number of dimensions. A model that works well with 1,000 samples in 10 dimensions might
                            need billions or even trillions of samples to perform similarly in 100 dimensions. This is why
                            "big data" is so crucial for modern AI.
                        </p>
                    </div>
                </div>
            </section>

            <!-- Solutions and Strategies -->
            <section class="mb-12">
                <h2 class="text-2xl font-bold text-gray-900 mb-6">How Do We Fight the Curse?</h2>

                <div class="prose prose-lg max-w-none">
                    <p class="text-gray-700 mb-6">
                        Fortunately, we're not helpless. Over the years, data scientists have developed a powerful toolkit
                        to mitigate the curse. The goal isn't to eliminate dimensions, but to manage them intelligently.
                    </p>

                    <div class="grid md:grid-cols-2 gap-8 mb-6">
                        <div class="bg-white p-6 rounded-lg border shadow-sm">
                            <h3 class="text-lg font-semibold text-green-700 mb-3">Taming the Feature Space</h3>
                            <p class="text-sm text-gray-600 mb-4">These methods focus on reducing the number of
                                dimensions before they get to the model.</p>
                            <ul class="space-y-3 text-gray-700 text-sm">
                                <li><strong>Feature Selection:</strong> This is the most direct approach. We use statistical
                                    tests or domain knowledge to identify and keep only the most informative features. It's
                                    like a journalist focusing on the key facts of a story and ignoring the irrelevant
                                    details.</li>
                                <li><strong>Dimensionality Reduction:</strong> Techniques like Principal Component Analysis
                                    (PCA) transform the original features into a smaller set of new, synthetic features
                                    that capture most of the original data's variance. It's a way of summarizing the data
                                    without losing its essence.</li>
                            </ul>
                        </div>
                        <div class="bg-white p-6 rounded-lg border shadow-sm">
                            <h3 class="text-lg font-semibold text-blue-700 mb-3">Making Models Wiser</h3>
                            <p class="text-sm text-gray-600 mb-4">These methods help models navigate the sparse landscape
                                of high-dimensional data.</p>
                            <ul class="space-y-3 text-gray-700 text-sm">
                                <li><strong>Regularization:</strong> This is a game-changer. Techniques like Lasso (L1) and
                                    Ridge (L2) add a penalty to the model for complexity. Lasso is particularly useful as
                                    it can force the coefficients of less important features to zero, effectively
                                    performing automatic feature selection. Elastic Net combines both, offering a robust
                                    solution for correlated features.</li>
                                <li><strong>Different Models:</strong> Some models are naturally more resistant to the curse.
                                    Tree-based models like Random Forests and Gradient Boosting can often handle
                                    high-dimensional data better than distance-based models because they partition the
                                    space recursively.</li>
                            </ul>
                        </div>
                    </div>

                    <p class="text-gray-700">
                        The best strategy is often a combination of these approaches. By thoughtfully selecting features,
                        reducing dimensionality, and choosing the right model, we can turn the curse into a manageable
                        challenge.
                    </p>
                </div>
            </section>

            <!-- Conclusion -->
            <section class="mb-12">
                <h2 class="text-2xl font-bold text-gray-900 mb-6">A Final Thought</h2>

                <div class="bg-gray-800 text-white p-8 rounded-lg shadow-2xl">
                    <p class="text-lg leading-relaxed text-gray-300">
                        The curse of dimensionality is a fundamental barrier in machine learning, but it's also a source
                        of creative innovation. It forces us to think critically about our data, to be deliberate in our
                        modeling choices, and to appreciate the strange, counterintuitive geometry of high-dimensional
                        space.
                    </p>
                    <p class="text-gray-200 mt-4 font-medium">
                        The next time you're faced with a dataset with hundreds of features, don't just throw a model at
                        it. Remember the empty space, the meaningless distances, and the challenge of finding signal in
                        the noise. Tame your dimensions before they tame you.
                    </p>
                </div>
            </section>
        </div>
    </div>

    <!-- JavaScript Files -->
    <script src="js/utils.js"></script>
    <script src="js/d3-charts.js"></script>
    <script src="js/threejs-scene.js"></script>
    <script src="js/main.js"></script>
</body>

</html>